(window.webpackJsonp=window.webpackJsonp||[]).push([[33],{500:function(t,s,n){"use strict";n.r(s);var a=n(25),r=Object(a.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("p"),n("div",{staticClass:"table-of-contents"},[n("ul",[n("li",[n("a",{attrs:{href:"#使用集合进行去重"}},[t._v("使用集合进行去重")])]),n("li",[n("a",{attrs:{href:"#使用布隆过滤器进行-url-去重"}},[t._v("使用布隆过滤器进行 url 去重")])])])]),t._v("\n当你可以从网站上获取网页，也可以将网页中有效的信息提取出来以后，接下来你会做什么？我想它一定是一个肯定的答案『获取整个网站的内容』，毕竟只获取网站上一个网页的内容听起来和看起来都不是那么的高大上，只有将整个网站的内容提取出来它才能称得上爬虫这个有科技感和高大上的名字。"),n("p"),t._v(" "),n("p",[t._v("要获取整个网站的内容，首先需要通过一个网址来获取其他的网址，这个我们可以使用上节解析内容的知识，从当前网页中解析出所含有的链接，从而根据每个网页中对其他网页的连接一层层获取整个网站的内容。此时我们会遇到一个问题，就是多个网页中可能含有相同的网页链接，此时需要将这个相同的链接识别出来，毕竟我们不想浪费珍贵的服务器资源去重复读取和解析同一个网页，要解决这个问题就需要通过 "),n("strong",[t._v("URL 去重")]),t._v("来实现。")]),t._v(" "),n("p",[t._v("在 Python 中 URL 去重可以通过以下几个方式来实现：")]),t._v(" "),n("ol",[n("li",[t._v("将 URL 保存在集合 (set) 中，使用集合的特性来去重。")]),t._v(" "),n("li",[t._v("使用布隆过滤器来对 URL 去重。")])]),t._v(" "),n("blockquote",[n("p",[t._v("对 URL 去重，还有将 URL 使用 MD5 等方法哈希后保存在 set 中的方法，原理与直接保存在 set 中相同，只是节省了内存空间。")])]),t._v(" "),n("h2",{attrs:{id:"使用集合进行去重"}},[t._v("使用集合进行去重")]),t._v(" "),n("p",[t._v("使用集合进行去重的优点是方便无需编写代码直接使用 python 内置的数据类型 set 即可，缺点是占用内存空间，虽然可以通过 MD5 等哈希算法来减少内存的占用但是当 url 的数量达到一定数量级的时候还是会占用大量的内存空间。")]),t._v(" "),n("p",[t._v("使用集合进行 url 去重时，只需在每次需要爬取该 url 时判断该 url 是否在集合中，若不在获取网页信息并将该 url 放入集合中，若存在则跳过该 url 即可。")]),t._v(" "),n("p",[t._v("当前使用的代码如下：")]),t._v(" "),n("div",{staticClass:"language-text line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("def __find_url(self, html):\n        for link in html.find_all(name='a', href=re.compile(r'https?://list|item.szlcsc.+')):\n            if len(self.__url_set) > self.__max_url_count:\n                return\n            url = link.get('href')\n            if url not in self.__url_set:\n                self.__url_set.add(url)\n                self.__url_queue.put(url)\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br")])]),n("h2",{attrs:{id:"使用布隆过滤器进行-url-去重"}},[t._v("使用布隆过滤器进行 url 去重")]),t._v(" "),n("p",[t._v("布隆过滤器在空间和时间上具有巨大的优势，它实际上是一个很长的二进制向量和一系列随机映射函数，因此占用的内存空间是固定的不会随 url 的增长而增长。同时它的确定也很明显有一定的误识别率且无法从布隆过滤器中删除已经添加的元素。")]),t._v(" "),n("p",[t._v("在 GitHub 上已经有人使用 python 实现了布隆过滤器，我们只需要直接使用该代码即可，布隆滤波器 "),n("a",{attrs:{href:"https://link.zhihu.com/?target=https%3A//github.com/jaybaird/python-bloomfilter",target:"_blank",rel:"noopener noreferrer"}},[t._v("源码"),n("OutboundLink")],1),t._v("。")]),t._v(" "),n("p",[t._v("将其应用于 url 去重的示例代码如下：")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__find_url")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" current_url"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" html"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" link "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" html"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("find_all"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" href"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("re"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'https?://list|item.szlcsc.+'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            url "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" link"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'href'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" url "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bloomfilter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__url_queue"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("qsize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__max_url_count"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bloomfilter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("url"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__url_queue"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("url"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__url_queue"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("current_url"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br"),n("span",{staticClass:"line-number"},[t._v("9")]),n("br"),n("span",{staticClass:"line-number"},[t._v("10")]),n("br")])]),n("p",[t._v("由于布隆过滤器存在一定的误算率「随着存入的元素数量增加，误算率随之增加」，因此布隆过滤器不适用于大量网页且对数据要求比较严格的场合。")]),t._v(" "),n("p",[t._v("在大多数场合我们使用集合来对 url 去重已经足够使用了，以一个 url 平均长度 100 字节来算，一千万条 url 使用集合进行去重所需要用到的内存空间不过也就是 1G，对现在的服务器或台式机来说应该不算太大的压力，且一千万的 url 已经算比较大的网站了。当 url 大于这个数的时候我想对数据的准确性也就要求不是那么高了。")])])}),[],!1,null,null,null);s.default=r.exports}}]);